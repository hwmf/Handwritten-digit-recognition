{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu_BOVHZaHiX"
   },
   "source": [
    "# **IA: Handwritten digit recognition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rPOUBU-a4LZ"
   },
   "source": [
    "## *import libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0k7kM00wdltv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn, optim\n",
    "from time import time\n",
    "from PySide2.QtWidgets import QApplication, QMainWindow, QPushButton, QTextEdit\n",
    "from PySide2.QtGui import QImage, QPainter, QMouseEvent, QPen, QPaintEvent\n",
    "from PySide2.QtCore import Qt, QPoint\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V8vE4Pzzfa_Z"
   },
   "outputs": [],
   "source": [
    "train_data = tv.datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YUa55L6wi2JH"
   },
   "outputs": [],
   "source": [
    "#train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ll0UlbKBjqeX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '5')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# un test\n",
    "plt.imshow(train_data[0][0].view(28,28))\n",
    "plt.title(train_data[0][1]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQwtPvSi1HhC"
   },
   "source": [
    "# *Charger le data dans le dataloaders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TxFou1b-1Uhn"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train, valid =torch.utils.data.random_split(train_data,[50000,10000])\n",
    "train_loader= torch.utils.data.DataLoader(train, batch_size=batch_size)\n",
    "valid_loader= torch.utils.data.DataLoader(valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fInM6cZPD3Pw"
   },
   "source": [
    "# *test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MG_QVAl7OJjO"
   },
   "outputs": [],
   "source": [
    "#for images, labels in train_loader:\n",
    "  #grid = tv.utils.make_grid(images)\n",
    "  #plt.imshow(grid.numpy().transpose((1,2,0)))\n",
    "  #plt.title(labels.numpy())\n",
    "  #plt.show()\n",
    "  \n",
    "  #break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrfkEjovWuMk"
   },
   "source": [
    "# *feature extraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OUK3P5U2JAF6"
   },
   "outputs": [],
   "source": [
    "\n",
    "def feature_extraction(image,nbr):\n",
    "      pnc=0\n",
    "      pni=0 \n",
    "      \n",
    "      ftr1=0\n",
    "      ftr2=0\n",
    "      ftr3=0\n",
    "      \n",
    "      for j in range(28):\n",
    "        for i in range (28):\n",
    "          if (image[nbr][0][0][j][i]==0):\n",
    "             pni =pni+1  \n",
    "  \n",
    "      \n",
    "      c=image[nbr][0]\n",
    "      b=c.view(1,1,16,7,7) \n",
    "      array1=[] \n",
    "      array2=[] \n",
    "      array3=[] \n",
    "      x=[]  \n",
    "      y=[]  \n",
    "      for k in range(16):\n",
    "        for j in range(7):\n",
    "          for i in range(7):\n",
    "           if (b[0][0][k][j][i]==0):\n",
    "            pnc=pnc+1  \n",
    "           if (b[0][0][k][j][i]!=0):\n",
    "            y.append(i)\n",
    "            x.append(j)\n",
    "        ftr1=pnc/pni \n",
    "        array1.append(ftr1) \n",
    "        p=0 \n",
    "        from scipy.stats import linregress\n",
    "        if (x!=[]):\n",
    "         p=linregress(x, y).slope \n",
    "         ftr2=2*p/(1+p*p) \n",
    "         ftr3=(1-p*p)/(1+p*p) \n",
    "        else : \n",
    "         ftr2=0\n",
    "         ftr3=1\n",
    "         \n",
    "        array2.append(ftr2)\n",
    "        array3.append(ftr3)\n",
    "      return (array1,array2,array3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jfzNJJRH3rjV"
   },
   "outputs": [],
   "source": [
    "#test\n",
    "#feature_extraction(train_data,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8B0PTcKPVWi"
   },
   "source": [
    "# *Tensor de taill 16x3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Eo9MqB44Phbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0763,  0.0000,  1.0000],\n",
      "        [ 0.1526,  0.0000,  1.0000],\n",
      "        [ 0.2290,  0.0000,  1.0000],\n",
      "        [ 0.3053,  0.0000,  1.0000],\n",
      "        [ 0.3520, -0.1881,  0.9821],\n",
      "        [ 0.4019, -0.0522,  0.9986],\n",
      "        [ 0.4564, -0.1016,  0.9948],\n",
      "        [ 0.5016,  0.0738,  0.9973],\n",
      "        [ 0.5421, -0.0402,  0.9992],\n",
      "        [ 0.6012,  0.0096,  1.0000],\n",
      "        [ 0.6667,  0.0658,  0.9978],\n",
      "        [ 0.7321,  0.0751,  0.9972],\n",
      "        [ 0.7960,  0.0233,  0.9997],\n",
      "        [ 0.8676,  0.0240,  0.9997],\n",
      "        [ 0.9299,  0.0654,  0.9979],\n",
      "        [ 1.0000,  0.0702,  0.9975]])\n",
      "tensor([[ 0.0763,  0.1526,  0.2290,  0.3053,  0.3520,  0.4019,  0.4564,  0.5016,\n",
      "          0.5421,  0.6012,  0.6667,  0.7321,  0.7960,  0.8676,  0.9299,  1.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.1881, -0.0522, -0.1016,  0.0738,\n",
      "         -0.0402,  0.0096,  0.0658,  0.0751,  0.0233,  0.0240,  0.0654,  0.0702],\n",
      "        [ 1.0000,  1.0000,  1.0000,  1.0000,  0.9821,  0.9986,  0.9948,  0.9973,\n",
      "          0.9992,  1.0000,  0.9978,  0.9972,  0.9997,  0.9997,  0.9979,  0.9975]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(list(feature_extraction(train_data,4))) \n",
    "print (torch.transpose(x, 0, 1)) #tensor sous forme [16*3]\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "L0SUPYh4IATY"
   },
   "outputs": [],
   "source": [
    "train , val =torch.utils.data.random_split(train_data,[50000,10000])\n",
    "train_loader=torch.utils.data.DataLoader(train,batch_size=24)\n",
    "val_loader=torch.utils.data.DataLoader(val,batch_size=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q27WE7L1WBEl"
   },
   "source": [
    "# *création du réseau de neuron*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=10, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [64, 32]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(train_loader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck3g3CzYXb-f"
   },
   "source": [
    "# *Core Training Process*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.6381996843084379\n",
      "Epoch 1 - Training loss: 0.25450515456411865\n",
      "Epoch 2 - Training loss: 0.18598786250673835\n",
      "Epoch 3 - Training loss: 0.1467633237622074\n",
      "Epoch 4 - Training loss: 0.12085239167974085\n",
      "Epoch 5 - Training loss: 0.10226621743579414\n",
      "Epoch 6 - Training loss: 0.08796641830730989\n",
      "Epoch 7 - Training loss: 0.07639633603006353\n",
      "Epoch 8 - Training loss: 0.06675547186337544\n",
      "Epoch 9 - Training loss: 0.058857164943595004\n",
      "Epoch 10 - Training loss: 0.0521148114651112\n",
      "Epoch 11 - Training loss: 0.04634691768226935\n",
      "Epoch 12 - Training loss: 0.0411090055376725\n",
      "Epoch 13 - Training loss: 0.036668390465497316\n",
      "Epoch 14 - Training loss: 0.032409867360454085\n",
      "\n",
      "Training Time (in minutes) = 3.179227431615194\n"
     ]
    }
   ],
   "source": [
    "SGDoptimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "time0 = time()\n",
    "epochs = 15\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        SGDoptimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        SGDoptimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(train_loader)))\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(img, ps):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit = 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(val_loader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "ps = torch.exp(logps)\n",
    "probab = list(ps.numpy()[0])\n",
    "print(\"Predicted Digit =\", probab.index(max(probab)))\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Fa6a-iG6APN"
   },
   "source": [
    "## *sauvgarder notre model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "d_1m-AxXp6G2"
   },
   "outputs": [],
   "source": [
    "# Specify a path\n",
    "PATH = \"projet_ia2.pt\"\n",
    "\n",
    "# Save\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJ_l68xApu09"
   },
   "source": [
    "# ***l'interface graphique:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NRMk2DUeKOL-"
   },
   "outputs": [],
   "source": [
    "def prepare_image(path: str):\n",
    "    \"\"\"\n",
    "    Converting image to MNIST dataset format\n",
    "    \"\"\"\n",
    "\n",
    "    im = Image.open(path).convert('L')\n",
    "    width = float(im.size[0])\n",
    "    height = float(im.size[1])\n",
    "    new_image = Image.new('L', (28, 28), (255))  # creates white canvas of 28x28 pixels\n",
    "\n",
    "    if width > height:  # check which dimension is bigger\n",
    "        # Width is bigger. Width becomes 20 pixels.\n",
    "        nheight = int(round((20.0 / width * height), 0))  # resize height according to ratio width\n",
    "        if (nheight == 0):  # rare case but minimum is 1 pixel\n",
    "            nheight = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((20, nheight), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wtop = int(round(((28 - nheight) / 2), 0))  # calculate horizontal position\n",
    "        new_image.paste(img, (4, wtop))  # paste resized image on white canvas\n",
    "    else:\n",
    "        # Height is bigger. Heigth becomes 20 pixels.\n",
    "        nwidth = int(round((20.0 / height * width), 0))  # resize width according to ratio height\n",
    "        if (nwidth == 0):  # rare case but minimum is 1 pixel\n",
    "            nwidth = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((nwidth, 20), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wleft = int(round(((28 - nwidth) / 2), 0))  # caculate vertical pozition\n",
    "        new_image.paste(img, (wleft, 4))  # paste resized image on white canvas\n",
    "\n",
    "    pixels = list(new_image.getdata())  # get pixel values\n",
    "    pixels_normalized = [(255 - x) * 1.0 / 255.0 for x in pixels]\n",
    "\n",
    "    # Need adequate shape\n",
    "    adequate_shape = np.reshape(pixels_normalized, (1, 28, 28))\n",
    "    output = torch.FloatTensor(adequate_shape).unsqueeze(0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Ov8LK0YZ6G0P"
   },
   "outputs": [],
   "source": [
    "\n",
    "PATH = \"projet_ia2.pt\"\n",
    "NET = model\n",
    "NET.load_state_dict(torch.load(PATH))\n",
    "NET.eval()\n",
    "\n",
    "\n",
    "class Window(QMainWindow):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setFixedSize(400, 400)\n",
    "        self.setWindowTitle('Handwritten digit recognition')\n",
    "\n",
    "        self.image = QImage(self.size(), QImage.Format_RGB32)\n",
    "        self.image.fill(Qt.white)\n",
    "\n",
    "        self.init_text()\n",
    "\n",
    "        self.drawing = False\n",
    "        self.brush_size = 8\n",
    "        self.brush_color = Qt.black\n",
    "        self.last_point = QPoint()\n",
    "\n",
    "        self.init_btn_clear()\n",
    "        self.init_btn_recognize()\n",
    "\n",
    "    def mousePressEvent(self, event: QMouseEvent):\n",
    "        if event.button() == Qt.LeftButton:\n",
    "            self.drawing = True\n",
    "            self.last_point = event.pos()\n",
    "\n",
    "    def mouseMoveEvent(self, event: QMouseEvent):\n",
    "        if (event.buttons() & Qt.LeftButton) & self.drawing:\n",
    "            painter = QPainter(self.image)\n",
    "            painter.setPen(QPen(self.brush_color, self.brush_size, Qt.SolidLine, Qt.RoundCap, Qt.RoundJoin))\n",
    "            painter.drawLine(self.last_point, event.pos())\n",
    "            self.last_point = event.pos()\n",
    "            self.update()\n",
    "\n",
    "    def mouseReleaseEvent(self, event: QMouseEvent):\n",
    "        if event.button() == Qt.LeftButton:\n",
    "            self.drawing = False\n",
    "\n",
    "    def paintEvent(self, event: QPaintEvent):\n",
    "        canvas_painter = QPainter(self)\n",
    "        canvas_painter.drawImage(self.rect(), self.image, self.image.rect())\n",
    "\n",
    "    def init_btn_clear(self):\n",
    "        btn = QPushButton('Clear', self)\n",
    "        btn.resize(80, 25)\n",
    "        btn.move(50, 340)\n",
    "        btn.show()\n",
    "        btn.clicked.connect(self.clear)\n",
    "\n",
    "    def clear(self):\n",
    "        self.image.fill(Qt.white)\n",
    "        self.text.setText('')\n",
    "        self.update()\n",
    "\n",
    "    def init_btn_recognize(self):\n",
    "        btn = QPushButton('Recognize', self)\n",
    "        btn.resize(80, 25)\n",
    "        btn.move(150, 340)\n",
    "        btn.show()\n",
    "        btn.clicked.connect(self.recognize)\n",
    "\n",
    "    def init_text(self):\n",
    "        self.text = QTextEdit(self)\n",
    "        self.text.setReadOnly(True)\n",
    "        self.text.setLineWrapMode(QTextEdit.NoWrap)\n",
    "        self.text.insertPlainText('')\n",
    "        font = self.text.font()\n",
    "        font.setFamily('Rockwell')\n",
    "        font.setPointSize(25)\n",
    "        self.text.setFont(font)\n",
    "        self.text.resize(50, 50)\n",
    "        self.text.move(266, 324)\n",
    "\n",
    "    def recognize(self):\n",
    "        # Convert to image\n",
    "        image = self.image.convertToFormat(QImage.Format_ARGB32)\n",
    "        width = image.width()\n",
    "        height = image.height()\n",
    "        ptr = image.constBits()\n",
    "        arr = np.frombuffer(ptr, np.uint8).reshape((height, width, 4))\n",
    "        im = Image.fromarray(arr[..., :3])\n",
    "        im.save('dataset/img.png')\n",
    "\n",
    "        # Evaluate net and show result\n",
    "        input_img = prepare_image('dataset/img.png')\n",
    "        prediction = torch.argmax(NET(input_img)).item()\n",
    "        self.text.setText(' '+str(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10rfzoI_K6kT"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "app = QApplication(sys.argv)\n",
    "window = Window()\n",
    "window.show()\n",
    "app.exec_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled1.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
